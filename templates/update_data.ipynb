{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update data\n",
    "\n",
    "This notebook downlads recent GitHub activity for a number of organizations.\n",
    "\n",
    "It will extract all issues, PRs, and comments that were updated within a\n",
    "window of interest. It will then save them to disk as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "parameters",
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "fmt = \"{:%Y-%m-%d}\"\n",
    "\n",
    "# Can optionally use number of days to choose dates\n",
    "n_days = 20\n",
    "end_date = fmt.format(pd.datetime.today())\n",
    "start_date = fmt.format(pd.datetime.today() - timedelta(days=n_days))\n",
    "\n",
    "github_orgs = [\"jupyterhub\", \"jupyter\", \"jupyterlab\", \"jupyter-widgets\", \"ipython\", \"binder-examples\", \"nteract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of days to include in plots\n",
    "n_days = (pd.to_datetime(end_date) - pd.to_datetime(start_date)).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_query = \"\"\"\\\n",
    "        comments(last: 50) {\n",
    "          edges {\n",
    "            node {\n",
    "              authorAssociation\n",
    "              createdAt\n",
    "              updatedAt\n",
    "              url\n",
    "              author {\n",
    "                login\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "base_elements = \"\"\"\\\n",
    "        state\n",
    "        id\n",
    "        title\n",
    "        url\n",
    "        createdAt\n",
    "        updatedAt\n",
    "        closedAt\n",
    "        number\n",
    "        authorAssociation\n",
    "        author {\n",
    "          login\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "gql_template = \"\"\"\\\n",
    "{{\n",
    "  search({query}) {{\n",
    "    issueCount\n",
    "    pageInfo {{\n",
    "        endCursor\n",
    "        hasNextPage\n",
    "    }}\n",
    "    nodes {{\n",
    "      ... on PullRequest {{\n",
    "        {base_elements}\n",
    "        mergedBy {{\n",
    "          login\n",
    "        }}\n",
    "        {comments}\n",
    "      }}\n",
    "      ... on Issue {{\n",
    "        {base_elements}\n",
    "        {comments}\n",
    "      }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our query object that we'll re-use for github search\n",
    "class GitHubGraphQlQuery():\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "        self.headers = {\"Authorization\": \"Bearer %s\" % os.environ['GITHUB_ACCESS_TOKEN']}\n",
    "        self.gql_template = gql_template\n",
    "\n",
    "    def request(self, n_pages=100, n_per_page=50):\n",
    "        self.raw_data = []\n",
    "        for ii in range(n_pages):\n",
    "            search_query = [\"first: %s\" % n_per_page, 'query: \"%s\"' % self.query, 'type: ISSUE']\n",
    "            if ii != 0:\n",
    "                search_query.append('after: \"%s\"' % pageInfo['endCursor'])\n",
    "\n",
    "            this_query = self.gql_template.format(\n",
    "                query=', '.join(search_query),\n",
    "                comments=comments_query,\n",
    "                base_elements=base_elements\n",
    "            )\n",
    "            request = requests.post('https://api.github.com/graphql', json={'query': this_query}, headers=self.headers)\n",
    "            if request.status_code != 200:\n",
    "                raise Exception(\"Query failed to run by returning code of {}. {}\".format(request.status_code, this_query))\n",
    "            if \"errors\" in request.json().keys():\n",
    "                raise Exception(\"Query failed to run with error {}. {}\".format(request.json()['errors'], this_query))\n",
    "            self.request = request\n",
    "\n",
    "            # Parse the response\n",
    "            json = request.json()['data']['search']\n",
    "            if ii == 0:\n",
    "                print(\"Found {} items, which will take {} pages\".format(json['issueCount'], int(np.ceil(json['issueCount'] / n_per_page))))\n",
    "            self.raw_data.append(json)\n",
    "            pageInfo = json['pageInfo']\n",
    "            self.last_query = this_query\n",
    "            if pageInfo['hasNextPage'] is False:\n",
    "                break\n",
    "        \n",
    "        if self.raw_data[0]['issueCount'] == 0:\n",
    "            print(\"Found no entries for query {}\".format(self.query))\n",
    "            self.data = None\n",
    "            return\n",
    "        \n",
    "        # Add some extra fields\n",
    "        self.data = pd.DataFrame([jj for ii in self.raw_data for jj in ii['nodes']])\n",
    "        self.data['author'] = self.data['author'].map(lambda a: a['login'] if a is not None else a)\n",
    "        self.data['org'] = self.data['url'].map(lambda a: a.split('/')[3])\n",
    "        self.data['repo'] = self.data['url'].map(lambda a: a.split('/')[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments(comments):\n",
    "    list_of_comments = [ii['edges'] for ii in comments]\n",
    "    comments = [jj['node'] for ii in list_of_comments for jj in ii]\n",
    "    comments = pd.DataFrame(comments)\n",
    "    comments['author'] = comments['author'].map(lambda a: a['login'] if a is not None else a)\n",
    "    \n",
    "    # Parse some data about the comments\n",
    "    url_parts = [ii.split('/') for ii in comments['url'].values]\n",
    "    url_parts = np.array([(ii[3], ii[4], ii[6]) for ii in url_parts])\n",
    "    orgs, repos, url_parts = url_parts.T\n",
    "\n",
    "    issue_id = [ii.split('#')[0] for ii in url_parts]\n",
    "    comment_id = [ii.split('-')[-1] for ii in url_parts]\n",
    "\n",
    "    # Assign new variables\n",
    "    comments['org'] = orgs\n",
    "    comments['repo'] = repos\n",
    "    comments['issue_id'] = issue_id\n",
    "    comments['id'] = comment_id\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GitHub activity\n",
    "\n",
    "Jupyter also has lots of activity across GitHub repositories. The following sections contain\n",
    "overviews of recent activity across the following GitHub organizations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 167 items, which will take 4 pages\n",
      "Found 245 items, which will take 5 pages\n",
      "Found 198 items, which will take 4 pages\n",
      "Found 64 items, which will take 2 pages\n",
      "Found 51 items, which will take 2 pages\n",
      "Found 2 items, which will take 1 pages\n",
      "Found 190 items, which will take 4 pages\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for org in github_orgs:\n",
    "    query_issues = f\"is:issue user:{org} updated:{start_date}..{end_date}\"\n",
    "    ghq = GitHubGraphQlQuery(query_issues)\n",
    "    ghq.request()\n",
    "    if ghq.data is None:\n",
    "        continue\n",
    "    responses.append(ghq)\n",
    "\n",
    "issues = pd.concat([ii.data for ii in responses])\n",
    "issues_comments = issues.pop(\"comments\")\n",
    "issues_comments = extract_comments(issues_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update PRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 78 items, which will take 2 pages\n",
      "Found 99 items, which will take 2 pages\n",
      "Found 94 items, which will take 2 pages\n",
      "Found 36 items, which will take 1 pages\n",
      "Found 6 items, which will take 1 pages\n",
      "Found 1 items, which will take 1 pages\n",
      "Found 60 items, which will take 2 pages\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for org in github_orgs:\n",
    "    query_prs = f\"is:pr user:{org} created:{start_date}..{end_date}\"\n",
    "    ghq = GitHubGraphQlQuery(query_prs)\n",
    "    ghq.request()\n",
    "    if ghq.data is None:\n",
    "        continue\n",
    "    responses.append(ghq)\n",
    "    \n",
    "prs = pd.concat([ii.data for ii in responses])\n",
    "prs_comments = prs.pop('comments')\n",
    "prs_comments = extract_comments(prs_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a PR-specific field for closed PRs\n",
    "prs['mergedBy'] = prs['mergedBy'].map(lambda a: a['login'] if a is not None else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.concat([prs_comments, issues_comments])\n",
    "\n",
    "# Only keep the comments within our window of interest\n",
    "comments = comments.query('updatedAt > @start_date and updatedAt < @end_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prs.to_csv('./data/prs.csv')\n",
    "issues.to_csv('./data/issues.csv')\n",
    "comments.to_csv('./data/comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.query('updatedAt > @start_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
